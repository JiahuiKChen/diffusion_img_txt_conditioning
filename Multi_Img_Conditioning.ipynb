{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f169e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_unclip#text-guided-image-to-image-variation\n",
    "\n",
    "# NEED THIS TO SAVE TO BIGGER DRIVE\n",
    "import os\n",
    "# os.environ['HF_HOME'] = '/datastor1/jiahuikchen/hf_cache' # CS A40 box\n",
    "os.environ['HF_HOME'] = '/home/jc98685/hf_cache' # MIDI-02\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from diffusers import StableUnCLIPImg2ImgPipeline\n",
    "from diffusers.utils import load_image, logging\n",
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d756890",
   "metadata": {},
   "source": [
    "### Image and Text Conditioned UnClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bae762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0a5f58b10248a8a1af639734fcbbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_txt_pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-unclip\", torch_dtype=torch.float16, \n",
    ")\n",
    "img_txt_pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "img_txt_pipe = img_txt_pipe.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18110c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text and image generation\n",
    "# prompts_and_imgs is list of tuples (prompt, image)\n",
    "def gen_images_from_text_and_img(prompts_and_imgs):\n",
    "    for tup in prompts_and_imgs:\n",
    "        prompt = tup[0]\n",
    "        img = tup[1]\n",
    "        images = img_txt_pipe(img, prompt=prompt, num_images_per_prompt=4).images\n",
    "\n",
    "        f, axarr = plt.subplots(1,4, figsize=(20, 20))\n",
    "        axarr[0].imshow(images[0]); axarr[0].axis(\"off\"); axarr[0].set_title(prompt)\n",
    "        axarr[1].imshow(images[1]); axarr[1].axis(\"off\"); axarr[1].set_title(prompt)\n",
    "        axarr[2].imshow(images[2]); axarr[2].axis(\"off\"); axarr[2].set_title(prompt)\n",
    "        axarr[3].imshow(images[3]); axarr[3].axis(\"off\"); axarr[3].set_title(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a65c1",
   "metadata": {},
   "source": [
    "## Mixup\n",
    "\n",
    "Original paper repo:\n",
    "https://github.com/facebookresearch/mixup-cifar10  \n",
    "\n",
    "Most of below is copied from: https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py#L152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75988f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                             (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='/home/jc98685/hf_cache/data', train=True, download=True,\n",
    "                            transform=transform_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=args.batch_size,\n",
    "                                          shuffle=True, num_workers=8)\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "batch_idx, (inputs, targets) = list(enumerate(trainloader))[0]\n",
    "if use_cuda:\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "inputs, targets_a, targets_b, lam = mixup_data(inputs, targets,\n",
    "                                               args.alpha, use_cuda)\n",
    "\n",
    "# what do the inputs look like??? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
